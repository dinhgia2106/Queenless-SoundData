{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "import librosa.display\n",
    "import scipy.fftpack as fftpack\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'E:/Queenless/20k_audio_splitted_dataset/train'\n",
    "val_path = 'E:/Queenless/20k_audio_splitted_dataset/val'\n",
    "test_path = 'E:/Queenless/20k_audio_splitted_dataset/test'\n",
    "\n",
    "output_dir = 'E:/Queenless/features'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from .pkl files...\n",
      "Loading val data from .pkl files...\n",
      "Loading test data from .pkl files...\n"
     ]
    }
   ],
   "source": [
    "def pre_emphasis(signal_in, alpha=0.97):\n",
    "    \"\"\"\n",
    "    Bước 1: Pre-emphasis - Lọc thông cao\n",
    "    \"\"\"\n",
    "    emphasized_signal = np.append(signal_in[0], signal_in[1:] - alpha * signal_in[:-1]) # y(t) = x(t) - alpha*x(t-1)\n",
    "    return emphasized_signal\n",
    "\n",
    "def framing(signal_in, sample_rate, frame_size=0.025, frame_stride=0.01):\n",
    "    \"\"\"\n",
    "    Bước 2: Chia khung (Framing)\n",
    "    - frame_size: kích thước khung (số giây)\n",
    "    - frame_stride: bước nhảy giữa các khung (số giây)\n",
    "    \"\"\"\n",
    "    frame_length = int(round(frame_size * sample_rate))\n",
    "    frame_step = int(round(frame_stride * sample_rate))\n",
    "    signal_length = len(signal_in)\n",
    "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step)) + 1\n",
    "\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    # Zero-padding nếu cần\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(signal_in, z)\n",
    "\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + \\\n",
    "              np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    return frames\n",
    "\n",
    "def windowing(frames):\n",
    "    \"\"\"\n",
    "    Bước 3: Áp dụng cửa sổ Hamming cho mỗi khung\n",
    "    \"\"\"\n",
    "    frame_length = frames.shape[1]\n",
    "    hamming = np.hamming(frame_length)\n",
    "    windowed_frames = frames * hamming\n",
    "    return windowed_frames\n",
    "\n",
    "def fft_frames(frames, NFFT=512):\n",
    "    \"\"\"\n",
    "    Bước 4: Tính FFT cho mỗi khung\n",
    "    \"\"\"\n",
    "    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))\n",
    "    return mag_frames\n",
    "\n",
    "def power_spectrum(mag_frames, NFFT=512):\n",
    "    \"\"\"\n",
    "    Bước 4.1: Tính phổ công suất của mỗi khung\n",
    "    \"\"\"\n",
    "    return (1.0 / NFFT) * (mag_frames ** 2)\n",
    "\n",
    "def mel_filterbank(sample_rate, NFFT, nfilt=26, low_freq=0, high_freq=None):\n",
    "    \"\"\"\n",
    "    Bước 5: Tạo Mel filterbank\n",
    "    \"\"\"\n",
    "    if high_freq is None:\n",
    "        high_freq = sample_rate / 2\n",
    "\n",
    "    # Chuyển Hz sang Mel\n",
    "    low_mel = 2595 * np.log10(1 + low_freq / 700.0)\n",
    "    high_mel = 2595 * np.log10(1 + high_freq / 700.0)\n",
    "    mel_points = np.linspace(low_mel, high_mel, nfilt + 2)\n",
    "    # Chuyển lại từ Mel sang Hz\n",
    "    hz_points = 700 * (10**(mel_points / 2595) - 1)\n",
    "    bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "\n",
    "    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus = int(bin[m - 1])   # giới hạn trái\n",
    "        f_m = int(bin[m])             # trung tâm\n",
    "        f_m_plus = int(bin[m + 1])    # giới hạn phải\n",
    "\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    return fbank\n",
    "\n",
    "# Hàm tính mfccs\n",
    "def compute_mfccs(signal_in, sample_rate, frame_size=0.025, frame_stride=0.01, \n",
    "                 pre_emph=0.97, NFFT=512, nfilt=26, num_ceps=13):\n",
    "    emphasized_signal = pre_emphasis(signal_in, pre_emph)\n",
    "    frames = framing(emphasized_signal, sample_rate, frame_size, frame_stride)\n",
    "    windowed_frames = windowing(frames)\n",
    "    mag_frames = fft_frames(windowed_frames, NFFT)\n",
    "    pow_frames = power_spectrum(mag_frames, NFFT)\n",
    "    fbank = mel_filterbank(sample_rate, NFFT, nfilt)\n",
    "    filter_banks = np.dot(pow_frames, fbank.T)\n",
    "    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "    log_fbank = np.log(filter_banks)\n",
    "    mfccs = fftpack.dct(log_fbank, type=2, axis=1, norm='ortho')[:, :num_ceps]\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_mfccs_features(directory, sample_rate=22050, output_dir=None, dataset_type='train'):\n",
    "    # Kiểm tra nếu file đã tồn tại, thì load lại\n",
    "    features_file = os.path.join(output_dir, f'mfccs_features_{dataset_type}.pkl')\n",
    "    labels_file = os.path.join(output_dir, f'mfccs_labels_{dataset_type}.pkl')\n",
    "    data_file = os.path.join(output_dir, f'mfccs_data_{dataset_type}.pkl')\n",
    "    \n",
    "    if os.path.exists(features_file) and os.path.exists(labels_file) and os.path.exists(data_file):\n",
    "        print(f\"Loading {dataset_type} data from .pkl files...\")\n",
    "        features = joblib.load(features_file)\n",
    "        labels = joblib.load(labels_file)\n",
    "        data = joblib.load(data_file)\n",
    "    else:\n",
    "        print(f\"Extracting {dataset_type} data...\")\n",
    "        start_time = time.time()\n",
    "        labels = []\n",
    "        features = []\n",
    "        data = []  # Thêm một biến để lưu dữ liệu đầu vào (có thể là tín hiệu, ví dụ như signal hoặc các thông tin khác)\n",
    "        \n",
    "        for label in ['Queen', 'NonQueen']:\n",
    "            path = os.path.join(directory, label)\n",
    "            for file in os.listdir(path):\n",
    "                file_path = os.path.join(path, file)\n",
    "                signal, sr = librosa.load(file_path, sr=sample_rate)\n",
    "                mfccs = compute_mfccs(signal_in=signal, sample_rate=sr)\n",
    "                mfccs_mean = np.mean(mfccs, axis=0)\n",
    "                features.append(mfccs_mean)\n",
    "                labels.append(label)\n",
    "                data.append(signal)  # Lưu tín hiệu âm thanh gốc hoặc dữ liệu khác nếu cần\n",
    "        \n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        data = np.array(data)  # Chuyển dữ liệu sang dạng numpy array nếu cần lưu\n",
    "        \n",
    "        if output_dir:\n",
    "            # Lưu từng đối tượng riêng biệt cho train/val/test\n",
    "            joblib.dump(features, features_file)\n",
    "            joblib.dump(labels, labels_file)\n",
    "            joblib.dump(data, data_file)  # Lưu tín hiệu âm thanh hoặc dữ liệu\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"MFCCs extraction time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return features, labels, data\n",
    "\n",
    "train_features_mfccs, train_labels_mfccs, train_data_mfccs = extract_mfccs_features(train_path, output_dir=output_dir, dataset_type='train')\n",
    "val_features_mfccs, val_labels_mfccs, val_data_mfccs = extract_mfccs_features(val_path, output_dir=output_dir, dataset_type='val')\n",
    "test_features_mfccs, test_labels_mfccs, test_data_mfccs = extract_mfccs_features(test_path, output_dir=output_dir, dataset_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14066, 13)\n",
      "(2010, 13)\n",
      "(4000, 13)\n"
     ]
    }
   ],
   "source": [
    "print(train_features_mfccs.shape)\n",
    "print(val_features_mfccs.shape)\n",
    "print(test_features_mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-84.59486884,   6.34044745,  -2.47652946,   5.6866161 ,\n",
       "        -6.0883983 ,   3.79775816,  -4.57106924,   2.68850088,\n",
       "        -2.55462216,   1.96805757,  -1.50288011,   0.65516511,\n",
       "        -0.11471445])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_mfccs[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features_mfccs_scaled = scaler.fit_transform(train_features_mfccs)\n",
    "val_features_mfccs_scaled = scaler.transform(val_features_mfccs)\n",
    "test_features_mfccs_scaled = scaler.transform(test_features_mfccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCCs - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 9.80 seconds\n",
      "Validation Accuracy: 91.99%\n",
      "Test Accuracy: 92.38%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, \n",
    "                                       min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                       max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                       bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, \n",
    "                                       warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "rf_classifier.fit(train_features_mfccs_scaled, train_labels_mfccs)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_predictions = rf_classifier.predict(val_features_mfccs_scaled)\n",
    "val_accuracy = accuracy_score(val_labels_mfccs, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "test_predictions = rf_classifier.predict(test_features_mfccs_scaled)\n",
    "test_accuracy_mfccs_rf = accuracy_score(test_labels_mfccs, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy_mfccs_rf * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning bootstrap=False from 92% to 92.38%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCCs - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 5.91 seconds\n",
      "Validation Accuracy (SVM with RBF Kernel): 93.63%\n",
      "Test Accuracy (SVM with RBF Kernel): 94.00%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "svm_rbf_classifier = SVC(C=10, kernel='rbf', degree=3, gamma=1, coef0=0.0, shrinking=True, \n",
    "                         probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                         max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=42)\n",
    "\n",
    "# Huấn luyện mô hình SVM\n",
    "svm_rbf_classifier.fit(train_features_mfccs, train_labels_mfccs)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_svm_rbf = svm_rbf_classifier.predict(val_features_mfccs)\n",
    "val_accuracy_svm_rbf = accuracy_score(val_labels_mfccs, val_predictions_svm_rbf)\n",
    "print(f\"Validation Accuracy (SVM with RBF Kernel): {val_accuracy_svm_rbf * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_svm_rbf = svm_rbf_classifier.predict(test_features_mfccs)\n",
    "test_accuracy_mfccs_svm = accuracy_score(test_labels_mfccs, test_predictions_svm_rbf)\n",
    "print(f\"Test Accuracy (SVM with RBF Kernel): {test_accuracy_mfccs_svm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 16.12 seconds\n",
      "Validation Accuracy (SVM with RBF Kernel): 93.03%\n",
      "Test Accuracy (SVM with RBF Kernel): 94.05%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Khởi tạo mô hình SVM với data scaling\n",
    "svm_rbf_classifier = SVC(C=10, kernel='rbf', degree=3, gamma=1, coef0=0.0, shrinking=True, \n",
    "                         probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                         max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=42)\n",
    "\n",
    "# Huấn luyện mô hình SVM\n",
    "svm_rbf_classifier.fit(train_features_mfccs_scaled, train_labels_mfccs)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_svm_rbf = svm_rbf_classifier.predict(val_features_mfccs_scaled)\n",
    "val_accuracy_svm_rbf = accuracy_score(val_labels_mfccs, val_predictions_svm_rbf)\n",
    "print(f\"Validation Accuracy (SVM with RBF Kernel): {val_accuracy_svm_rbf * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_svm_rbf = svm_rbf_classifier.predict(test_features_mfccs_scaled)\n",
    "scale_test_accuracy_mfccs_svm = accuracy_score(test_labels_mfccs, test_predictions_svm_rbf)\n",
    "print(f\"Test Accuracy (SVM with RBF Kernel): {scale_test_accuracy_mfccs_svm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data improve from 94% to 94.05%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCCs - LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.07 seconds\n",
      "Validation Accuracy (Logistic Regression): 70.85%\n",
      "Test Accuracy (Logistic Regression): 72.67%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Khởi tạo mô hình Logistic Regression\n",
    "lr_classifier = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "                                   class_weight=None, random_state=42, solver='liblinear', max_iter=100, multi_class='deprecated', \n",
    "                                   verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "# Huấn luyện mô hình Logistic Regression với dữ liệu đã chuẩn hóa\n",
    "lr_classifier.fit(train_features_mfccs, train_labels_mfccs)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_lr = lr_classifier.predict(val_features_mfccs)\n",
    "val_accuracy_lr = accuracy_score(val_labels_mfccs, val_predictions_lr)\n",
    "print(f\"Validation Accuracy (Logistic Regression): {val_accuracy_lr * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_lr = lr_classifier.predict(test_features_mfccs)\n",
    "test_accuracy_mfccs_lr = accuracy_score(test_labels_mfccs, test_predictions_lr)\n",
    "print(f\"Test Accuracy (Logistic Regression): {test_accuracy_mfccs_lr * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.04 seconds\n",
      "Validation Accuracy (Logistic Regression): 71.79%\n",
      "Test Accuracy (Logistic Regression): 73.28%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Khởi tạo mô hình Logistic Regression với data scaling\n",
    "lr_classifier = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "                                   class_weight=None, random_state=42, solver='liblinear', max_iter=100, multi_class='deprecated', \n",
    "                                   verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "# Huấn luyện mô hình Logistic Regression với dữ liệu đã chuẩn hóa\n",
    "lr_classifier.fit(train_features_mfccs_scaled, train_labels_mfccs)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_lr = lr_classifier.predict(val_features_mfccs_scaled)\n",
    "val_accuracy_lr = accuracy_score(val_labels_mfccs, val_predictions_lr)\n",
    "print(f\"Validation Accuracy (Logistic Regression): {val_accuracy_lr * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_lr = lr_classifier.predict(test_features_mfccs_scaled)\n",
    "scale_test_accuracy_mfccs_lr = accuracy_score(test_labels_mfccs, test_predictions_lr)\n",
    "print(f\"Test Accuracy (Logistic Regression): {scale_test_accuracy_mfccs_lr * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuned the LR model but still only has 73.28% accuracy -> problem with feature extraction method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCCs - ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 5.96 seconds\n",
      "Validation Accuracy (Extra Trees): 92.09%\n",
      "Test Accuracy (Extra Trees): 92.97%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Khởi tạo mô hình Extra Trees \n",
    "et_classifier = ExtraTreesClassifier(n_estimators=200, criterion='gini', max_depth=30, min_samples_split=2, min_samples_leaf=1, \n",
    "                                     min_weight_fraction_leaf=0.0, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                     bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, warm_start=False, \n",
    "                                     class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "et_classifier.fit(train_features_mfccs, train_labels_mfccs)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_et = et_classifier.predict(val_features_mfccs)\n",
    "val_accuracy_et = accuracy_score(val_labels_mfccs, val_predictions_et)\n",
    "print(f\"Validation Accuracy (Extra Trees): {val_accuracy_et * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_et = et_classifier.predict(test_features_mfccs)\n",
    "test_accuracy_mfccs_et = accuracy_score(test_labels_mfccs, test_predictions_et)\n",
    "print(f\"Test Accuracy (Extra Trees): {test_accuracy_mfccs_et * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 6.93 seconds\n",
      "Validation Accuracy (Extra Trees): 92.14%\n",
      "Test Accuracy (Extra Trees): 92.90%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Khởi tạo mô hình Extra Trees với data scaling\n",
    "et_classifier = ExtraTreesClassifier(n_estimators=200, criterion='gini', max_depth=30, min_samples_split=2, min_samples_leaf=1, \n",
    "                                     min_weight_fraction_leaf=0.0, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                     bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, warm_start=False, \n",
    "                                     class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "# Huấn luyện mô hình Extra Trees với dữ liệu đã chuẩn hóa\n",
    "et_classifier.fit(train_features_mfccs_scaled, train_labels_mfccs)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_et = et_classifier.predict(val_features_mfccs_scaled)\n",
    "val_accuracy_et = accuracy_score(val_labels_mfccs, val_predictions_et)\n",
    "print(f\"Validation Accuracy (Extra Trees): {val_accuracy_et * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_et = et_classifier.predict(test_features_mfccs_scaled)\n",
    "scale_test_accuracy_mfccs_et = accuracy_score(test_labels_mfccs, test_predictions_et)\n",
    "print(f\"Test Accuracy (Extra Trees): {scale_test_accuracy_mfccs_et * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tuning improved from 92.53% to 92.97%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCCs - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.04 seconds\n",
      "Validation Accuracy (KNN): 91.00%\n",
      "Test Accuracy (KNN): 91.57%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                      metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "# Huấn luyện mô hình KNN với dữ liệu đã chuẩn hóa\n",
    "knn_classifier.fit(train_features_mfccs, train_labels_mfccs)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_knn = knn_classifier.predict(val_features_mfccs)\n",
    "val_accuracy_knn = accuracy_score(val_labels_mfccs, val_predictions_knn)\n",
    "print(f\"Validation Accuracy (KNN): {val_accuracy_knn * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_knn = knn_classifier.predict(test_features_mfccs)\n",
    "test_accuracy_mfccs_knn = accuracy_score(test_labels_mfccs, test_predictions_knn)\n",
    "print(f\"Test Accuracy (KNN): {test_accuracy_mfccs_knn * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.03 seconds\n",
      "Validation Accuracy (KNN): 91.44%\n",
      "Test Accuracy (KNN): 92.75%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                      metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "# Huấn luyện mô hình KNN với dữ liệu đã chuẩn hóa\n",
    "knn_classifier.fit(train_features_mfccs_scaled, train_labels_mfccs)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_knn = knn_classifier.predict(val_features_mfccs_scaled)\n",
    "val_accuracy_knn = accuracy_score(val_labels_mfccs, val_predictions_knn)\n",
    "print(f\"Validation Accuracy (KNN): {val_accuracy_knn * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_knn = knn_classifier.predict(test_features_mfccs_scaled)\n",
    "scale_test_accuracy_mfccs_knn = accuracy_score(test_labels_mfccs, test_predictions_knn)\n",
    "print(f\"Test Accuracy (KNN): {scale_test_accuracy_mfccs_knn * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFTs (with pre-emphasis, framing, windowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_emphasis(signal_in, pre_emph=0.97):\n",
    "    \"\"\"\n",
    "    Áp dụng pre-emphasis để nhấn mạnh các tần số cao.\n",
    "    \"\"\"\n",
    "    return np.append(signal_in[0], signal_in[1:] - pre_emph * signal_in[:-1])\n",
    "\n",
    "def framing(signal_in, sample_rate, frame_size=0.025, frame_stride=0.01):\n",
    "    \"\"\"\n",
    "    Chia tín hiệu thành các frame có kích thước và bước nhảy xác định.\n",
    "    \"\"\"\n",
    "    frame_length = int(round(frame_size * sample_rate))\n",
    "    frame_step = int(round(frame_stride * sample_rate))\n",
    "    signal_length = len(signal_in)\n",
    "    num_frames = int(np.ceil(np.abs(signal_length - frame_length) / frame_step)) + 1\n",
    "\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(signal_in, z)\n",
    "\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + \\\n",
    "              np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    return frames\n",
    "\n",
    "def windowing(frames):\n",
    "    \"\"\"\n",
    "    Áp dụng cửa sổ Hamming cho mỗi frame.\n",
    "    \"\"\"\n",
    "    frame_length = frames.shape[1]\n",
    "    hamming = np.hamming(frame_length)\n",
    "    return frames * hamming\n",
    "\n",
    "def fft_frames(frames, NFFT=512):\n",
    "    \"\"\"\n",
    "    Tính FFT cho mỗi frame và lấy giá trị magnitude.\n",
    "    \"\"\"\n",
    "    return np.absolute(np.fft.rfft(frames, NFFT))\n",
    "\n",
    "def compute_fft_features(signal_in, sample_rate, frame_size=0.025, frame_stride=0.01, NFFT=512, apply_log=True):\n",
    "    \"\"\"\n",
    "    Tính toán đặc trưng FFT cho tín hiệu âm thanh:\n",
    "      - Pre-emphasis, Framing, Windowing.\n",
    "      - Tính FFT cho từng frame và lấy giá trị magnitude.\n",
    "      - Trung bình các frame để có vector đặc trưng ổn định.\n",
    "      - (Tùy chọn) Áp dụng log để giảm phạm vi giá trị.\n",
    "      \n",
    "    Trả về: vector đặc trưng có kích thước (NFFT/2+1,).\n",
    "    \"\"\"\n",
    "    emphasized_signal = pre_emphasis(signal_in)\n",
    "    frames = framing(emphasized_signal, sample_rate, frame_size, frame_stride)\n",
    "    windowed_frames = windowing(frames)\n",
    "    mag_frames = fft_frames(windowed_frames, NFFT)\n",
    "    fft_feature = np.mean(mag_frames, axis=0)  # Trung bình theo các frame\n",
    "    if apply_log:\n",
    "        fft_feature = np.log(fft_feature + 1e-8)  # Thêm epsilon để tránh log(0)\n",
    "    return fft_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pre_emphasis(signal_in, pre_emph=0.97):\n",
    "    \"\"\"\n",
    "    Áp dụng pre-emphasis để nhấn mạnh các tần số cao.\n",
    "    \"\"\"\n",
    "    return np.append(signal_in[0], signal_in[1:] - pre_emph * signal_in[:-1])\n",
    "\n",
    "def framing(signal_in, sample_rate, frame_size=0.025, frame_stride=0.01):\n",
    "    \"\"\"\n",
    "    Chia tín hiệu thành các frame có kích thước và bước nhảy xác định.\n",
    "    \"\"\"\n",
    "    frame_length = int(round(frame_size * sample_rate))\n",
    "    frame_step = int(round(frame_stride * sample_rate))\n",
    "    signal_length = len(signal_in)\n",
    "    \n",
    "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step)) + 1\n",
    "\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    pad_signal = np.pad(signal_in, (0, pad_signal_length - signal_length), mode='constant')\n",
    "\n",
    "    indices = np.arange(0, frame_length) + np.arange(0, num_frames * frame_step, frame_step)[:, None]\n",
    "    frames = pad_signal[indices.astype(np.int32)]\n",
    "    return frames\n",
    "\n",
    "def windowing(frames):\n",
    "    \"\"\"\n",
    "    Áp dụng cửa sổ Hamming cho mỗi frame.\n",
    "    \"\"\"\n",
    "    frame_length = frames.shape[1]\n",
    "    hamming = np.hamming(frame_length)\n",
    "    return frames * hamming\n",
    "\n",
    "def fft_frames(frames, NFFT=512):\n",
    "    \"\"\"\n",
    "    Tính FFT cho mỗi frame và lấy giá trị magnitude.\n",
    "    \"\"\"\n",
    "    return np.abs(np.fft.fft(frames, NFFT))[:, :NFFT//2+1]  # Chỉ lấy giá trị magnitude dương\n",
    "\n",
    "def compute_fft_features(signal_in, sample_rate, frame_size=0.025, frame_stride=0.01, NFFT=512, apply_log=True):\n",
    "    \"\"\"\n",
    "    Tính toán đặc trưng FFT cho tín hiệu âm thanh:\n",
    "      - Pre-emphasis, Framing, Windowing.\n",
    "      - Tính FFT cho từng frame và lấy giá trị magnitude.\n",
    "      - Trung bình các frame để có vector đặc trưng ổn định.\n",
    "      - (Tùy chọn) Áp dụng log để giảm phạm vi giá trị.\n",
    "    Trả về: vector đặc trưng có kích thước (NFFT/2+1,).\n",
    "    \"\"\"\n",
    "    emphasized_signal = pre_emphasis(signal_in)\n",
    "    frames = framing(emphasized_signal, sample_rate, frame_size, frame_stride)\n",
    "    windowed_frames = windowing(frames)\n",
    "    mag_frames = fft_frames(windowed_frames, NFFT)\n",
    "    fft_feature = np.mean(mag_frames, axis=0)  # Trung bình theo các frame\n",
    "    \n",
    "    # Áp dụng log nếu cần, đảm bảo không có log(0)\n",
    "    if apply_log:\n",
    "        fft_feature = np.log(np.maximum(fft_feature, 1e-8))  # Thêm epsilon để tránh log(0)\n",
    "    \n",
    "    return fft_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fft_features_from_directory(directory, sample_rate=22050, NFFT=512, output_dir=None, dataset_type='train'):\n",
    "    \"\"\"\n",
    "    Duyệt qua các file âm thanh trong thư mục và tính đặc trưng FFT cho mỗi file.\n",
    "    Giả sử trong directory có hai thư mục con: 'Queen' và 'NonQueen'.\n",
    "    \n",
    "    Trả về:\n",
    "      - features: mảng đặc trưng (mỗi đặc trưng có kích thước NFFT/2+1)\n",
    "      - labels: nhãn tương ứng.\n",
    "    \"\"\"\n",
    "    # Kiểm tra nếu file đã tồn tại, thì load lại\n",
    "    features_file = os.path.join(output_dir, f'fft3_features_{dataset_type}.pkl')\n",
    "    labels_file = os.path.join(output_dir, f'fft3_labels_{dataset_type}.pkl')\n",
    "    \n",
    "    if os.path.exists(features_file) and os.path.exists(labels_file):\n",
    "        print(f\"Loading {dataset_type} data from .pkl files...\")\n",
    "        features = joblib.load(features_file)\n",
    "        labels = joblib.load(labels_file)\n",
    "    else:\n",
    "        print(f\"Extracting {dataset_type} data...\")\n",
    "        start_time = time.time()\n",
    "        labels = []\n",
    "        features = []\n",
    "        \n",
    "        for label in ['Queen', 'NonQueen']:\n",
    "            path = os.path.join(directory, label)\n",
    "            for file in os.listdir(path):\n",
    "                file_path = os.path.join(path, file)\n",
    "                signal, sr = librosa.load(file_path, sr=sample_rate)\n",
    "                fft_feature = compute_fft_features(signal, sr, NFFT=NFFT)\n",
    "                features.append(fft_feature)\n",
    "                labels.append(label)\n",
    "        \n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Lưu dữ liệu vào file .pkl nếu chưa tồn tại\n",
    "        if output_dir:\n",
    "            joblib.dump(features, features_file)\n",
    "            joblib.dump(labels, labels_file)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"FFT extraction time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train data...\n",
      "FFT extraction time: 66.67 seconds\n",
      "Extracting val data...\n",
      "FFT extraction time: 9.76 seconds\n",
      "Extracting test data...\n",
      "FFT extraction time: 19.05 seconds\n"
     ]
    }
   ],
   "source": [
    "train_features_fft, train_labels_fft = load_fft_features_from_directory(train_path, sample_rate=22050, NFFT=512, output_dir=output_dir, dataset_type='train')\n",
    "val_features_fft, val_labels_fft = load_fft_features_from_directory(val_path, sample_rate=22050, NFFT=512, output_dir=output_dir, dataset_type='val')\n",
    "test_features_fft, test_labels_fft = load_fft_features_from_directory(test_path, sample_rate=22050, NFFT=512, output_dir=output_dir, dataset_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features_fft_scaled = scaler.fit_transform(train_features_fft)\n",
    "val_features_fft_scaled = scaler.transform(val_features_fft)\n",
    "test_features_fft_scaled = scaler.transform(test_features_fft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFTs - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.02 seconds\n",
      "KNN (FFT features) - Validation Accuracy: 93.28%\n",
      "KNN (FFT features) - Test Accuracy: 95.15%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(train_features_fft, train_labels_fft)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_knn = knn_classifier.predict(val_features_fft)\n",
    "test_accuracy_fft_knn = knn_classifier.predict(test_features_fft)\n",
    "print(f\"KNN (FFT features) - Validation Accuracy: {accuracy_score(val_labels_fft, val_pred_knn)*100:.2f}%\")\n",
    "print(f\"KNN (FFT features) - Test Accuracy: {accuracy_score(test_labels_fft, test_accuracy_fft_knn)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94.25% to 95.15%, pre_emph = 0.98 => 95.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.02 seconds\n",
      "KNN (FFT features) - Validation Accuracy: 93.03%\n",
      "KNN (FFT features) - Test Accuracy: 94.25%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(train_features_fft_scaled, train_labels_fft)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_knn = knn_classifier.predict(val_features_fft_scaled)\n",
    "test_accuracy_fft_knn = knn_classifier.predict(test_features_fft_scaled)\n",
    "print(f\"KNN (FFT features) - Validation Accuracy: {accuracy_score(val_labels_fft, val_pred_knn)*100:.2f}%\")\n",
    "print(f\"KNN (FFT features) - Test Accuracy: {accuracy_score(test_labels_fft, test_accuracy_fft_knn)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94.25%, pre_emph = 0.98 => 94.42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFTs - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 10.43 seconds\n",
      "Validation Accuracy (SVM with RBF Kernel): 97.71%\n",
      "Test Accuracy (SVM with RBF Kernel): 97.47%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "svm_rbf_classifier = SVC(C=100, kernel='rbf', degree=3, gamma=0.1, coef0=0.0, shrinking=True, \n",
    "                         probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                         max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=42)\n",
    "\n",
    "# Huấn luyện mô hình SVM\n",
    "svm_rbf_classifier.fit(train_features_fft, train_labels_fft)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_svm_rbf = svm_rbf_classifier.predict(val_features_fft)\n",
    "val_accuracy_svm_rbf = accuracy_score(val_labels_fft, val_predictions_svm_rbf)\n",
    "print(f\"Validation Accuracy (SVM with RBF Kernel): {val_accuracy_svm_rbf * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_svm_rbf = svm_rbf_classifier.predict(test_features_fft)\n",
    "test_accuracy_fft_svm = accuracy_score(test_labels_fft, test_predictions_svm_rbf)\n",
    "print(f\"Test Accuracy (SVM with RBF Kernel): {test_accuracy_fft_svm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97.47%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 32.25 seconds\n",
      "Validation Accuracy (SVM with RBF Kernel): 97.11%\n",
      "Test Accuracy (SVM with RBF Kernel): 97.82%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "svm_rbf_classifier = SVC(C=100, kernel='rbf', degree=3, gamma=0.1, coef0=0.0, shrinking=True, \n",
    "                         probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                         max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=42)\n",
    "\n",
    "# Huấn luyện mô hình SVM\n",
    "svm_rbf_classifier.fit(train_features_fft_scaled, train_labels_fft)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_svm_rbf = svm_rbf_classifier.predict(val_features_fft_scaled)\n",
    "val_accuracy_svm_rbf = accuracy_score(val_labels_fft, val_predictions_svm_rbf)\n",
    "print(f\"Validation Accuracy (SVM with RBF Kernel): {val_accuracy_svm_rbf * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_svm_rbf = svm_rbf_classifier.predict(test_features_fft_scaled)\n",
    "scale_test_accuracy_fft_svm = accuracy_score(test_labels_fft, test_predictions_svm_rbf)\n",
    "print(f\"Test Accuracy (SVM with RBF Kernel): {scale_test_accuracy_fft_svm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFTs - LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 8.09 seconds\n",
      "Logistic Regression (FFT features) - Validation Accuracy: 86.07%\n",
      "Logistic Regression (FFT features) - Test Accuracy: 86.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinhg\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "lr_classifier = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True, intercept_scaling=1, \n",
    "                                   class_weight=None, random_state=42, solver='lbfgs', max_iter=1500, multi_class='deprecated', \n",
    "                                   verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "lr_classifier.fit(train_features_fft, train_labels_fft)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_lr = lr_classifier.predict(val_features_fft)\n",
    "test_accuracy_fft_lr = lr_classifier.predict(test_features_fft)\n",
    "print(f\"Logistic Regression (FFT features) - Validation Accuracy: {accuracy_score(val_labels_fft, val_pred_lr)*100:.2f}%\")\n",
    "print(f\"Logistic Regression (FFT features) - Test Accuracy: {accuracy_score(test_labels_fft, test_accuracy_fft_lr)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 60.68 seconds\n",
      "Logistic Regression (FFT features) - Validation Accuracy: 85.57%\n",
      "Logistic Regression (FFT features) - Test Accuracy: 86.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinhg\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "lr_classifier = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True, intercept_scaling=1, \n",
    "                                   class_weight=None, random_state=42, solver='saga', max_iter=1000, multi_class='deprecated', \n",
    "                                   verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "lr_classifier.fit(train_features_fft_scaled, train_labels_fft)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_lr = lr_classifier.predict(val_features_fft_scaled)\n",
    "scale_test_accuracy_fft_lr = lr_classifier.predict(test_features_fft_scaled)\n",
    "print(f\"Logistic Regression (FFT features) - Validation Accuracy: {accuracy_score(val_labels_fft, val_pred_lr)*100:.2f}%\")\n",
    "print(f\"Logistic Regression (FFT features) - Test Accuracy: {accuracy_score(test_labels_fft, scale_test_accuracy_fft_lr)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning from 85.97% to 86.05%\n",
    "\n",
    "Using min max scaling -> 86.08%\n",
    "\n",
    "But Non-scaling data working better -> 86.33%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFTs - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 32.89 seconds\n",
      "Random Forest (FFT features) - Validation Accuracy: 93.58%\n",
      "Random Forest (FFT features) - Test Accuracy: 94.67%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                                       min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', \n",
    "                                       max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, \n",
    "                                       n_jobs=None, random_state=42, verbose=0, warm_start=False, class_weight=None, \n",
    "                                       ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "rf_classifier.fit(train_features_fft, train_labels_fft)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_rf = rf_classifier.predict(val_features_fft)\n",
    "test_accuracy_fft_rf = rf_classifier.predict(test_features_fft)\n",
    "print(f\"Random Forest (FFT features) - Validation Accuracy: {accuracy_score(val_labels_fft, val_pred_rf)*100:.2f}%\")\n",
    "print(f\"Random Forest (FFT features) - Test Accuracy: {accuracy_score(test_labels_fft, test_accuracy_fft_rf)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not tuning yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFTs - ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 102.19 seconds\n",
      "Validation Accuracy (Extra Trees): 94.88%\n",
      "Test Accuracy (Extra Trees): 95.73%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "et_classifier = ExtraTreesClassifier(n_estimators=200, criterion='gini', max_depth=30, min_samples_split=2, min_samples_leaf=1, \n",
    "                                     min_weight_fraction_leaf=0.0, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                     bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, warm_start=False, \n",
    "                                     class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "# Huấn luyện mô hình Extra Trees với dữ liệu đã chuẩn hóa\n",
    "et_classifier.fit(train_features_fft, train_labels_fft)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_et = et_classifier.predict(val_features_fft)\n",
    "val_accuracy_et = accuracy_score(val_labels_fft, val_predictions_et)\n",
    "print(f\"Validation Accuracy (Extra Trees): {val_accuracy_et * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_et = et_classifier.predict(test_features_fft)\n",
    "test_accuracy_fft_et = accuracy_score(test_labels_fft, test_predictions_et)\n",
    "print(f\"Test Accuracy (Extra Trees): {test_accuracy_fft_et * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 102.91 seconds\n",
      "Validation Accuracy (Extra Trees): 94.98%\n",
      "Test Accuracy (Extra Trees): 95.80%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "et_classifier = ExtraTreesClassifier(n_estimators=200, criterion='gini', max_depth=30, min_samples_split=2, min_samples_leaf=1, \n",
    "                                     min_weight_fraction_leaf=0.0, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                     bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, warm_start=False, \n",
    "                                     class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "# Huấn luyện mô hình Extra Trees với dữ liệu đã chuẩn hóa\n",
    "et_classifier.fit(train_features_fft_scaled, train_labels_fft)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_et = et_classifier.predict(val_features_fft_scaled)\n",
    "val_accuracy_et = accuracy_score(val_labels_fft, val_predictions_et)\n",
    "print(f\"Validation Accuracy (Extra Trees): {val_accuracy_et * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_et = et_classifier.predict(test_features_fft_scaled)\n",
    "scale_test_accuracy_fft_et = accuracy_score(test_labels_fft, test_predictions_et)\n",
    "print(f\"Test Accuracy (Extra Trees): {scale_test_accuracy_fft_et * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93.95% -> 95.80% beautifull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mfcc(file_path, n_mfcc=13):\n",
    "    y, sr = librosa.load(file_path, sr=16000) \n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, S=None, n_mfcc=n_mfcc, dct_type=2, norm='ortho', lifter=0, mel_norm='slaney')\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from .pkl files...\n",
      "Loading val data from .pkl files...\n",
      "Loading test data from .pkl files...\n"
     ]
    }
   ],
   "source": [
    "def extract_mfcc_features(directory, sample_rate=22050, output_dir=None, dataset_type='train', reduce_dimension=True):\n",
    "    # Kiểm tra nếu file đã tồn tại, thì load lại\n",
    "    features_file = os.path.join(output_dir, f'mfcc2_features_{dataset_type}.pkl')\n",
    "    labels_file = os.path.join(output_dir, f'mfcc2_labels_{dataset_type}.pkl')\n",
    "    data_file = os.path.join(output_dir, f'mfcc2_data_{dataset_type}.pkl')\n",
    "    \n",
    "    if os.path.exists(features_file) and os.path.exists(labels_file) and os.path.exists(data_file):\n",
    "        print(f\"Loading {dataset_type} data from .pkl files...\")\n",
    "        features = joblib.load(features_file)\n",
    "        labels = joblib.load(labels_file)\n",
    "        data = joblib.load(data_file)\n",
    "    else:\n",
    "        print(f\"Extracting {dataset_type} data...\")\n",
    "        start_time = time.time()\n",
    "        labels = []\n",
    "        features = []\n",
    "        data = []  # Thêm một biến để lưu dữ liệu đầu vào (có thể là tín hiệu, ví dụ như signal hoặc các thông tin khác)\n",
    "        \n",
    "        for label in ['Queen', 'NonQueen']:\n",
    "            path = os.path.join(directory, label)\n",
    "            for file in os.listdir(path):\n",
    "                file_path = os.path.join(path, file)\n",
    "                mfcc = compute_mfcc(file_path=file_path, n_mfcc=70)  # Chiết xuất MFCC cho mỗi tệp âm thanh\n",
    "                features.append(mfcc)  # Lưu ma trận MFCC cho mỗi tệp âm thanh\n",
    "                labels.append(label)\n",
    "        \n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        data = np.array(data)  # Chuyển dữ liệu sang dạng numpy array nếu cần lưu\n",
    "        \n",
    "        # Nếu cần giảm chiều theo axis=2\n",
    "        if reduce_dimension:\n",
    "            features = np.mean(features, axis=2)\n",
    "        \n",
    "        if output_dir:\n",
    "            # Lưu từng đối tượng riêng biệt cho train/val/test\n",
    "            joblib.dump(features, features_file)\n",
    "            joblib.dump(labels, labels_file)\n",
    "            joblib.dump(data, data_file)  # Lưu tín hiệu âm thanh hoặc dữ liệu\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"MFCC extraction time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return features, labels, data\n",
    "\n",
    "# Gọi hàm với tham số reduce_dimension=True để giảm chiều ngay khi gọi hàm\n",
    "train_features_mfcc, train_labels_mfcc, train_data_mfcc = extract_mfcc_features(train_path, output_dir=output_dir, dataset_type='train')\n",
    "val_features_mfcc, val_labels_mfcc, val_data_mfcc = extract_mfcc_features(val_path, output_dir=output_dir, dataset_type='val')\n",
    "test_features_mfcc, test_labels_mfcc, test_data_mfcc = extract_mfcc_features(test_path, output_dir=output_dir, dataset_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14066, 120)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_mfcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features_mfcc_scaled = scaler.fit_transform(train_features_mfcc)\n",
    "val_features_mfcc_scaled = scaler.transform(val_features_mfcc)\n",
    "test_features_mfcc_scaled = scaler.transform(test_features_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 33.88 seconds\n",
      "Validation Accuracy (SVM with RBF Kernel): 92.84%\n",
      "Test Accuracy (SVM with RBF Kernel): 92.80%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "svm_rbf_classifier = SVC(C=8.31, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, \n",
    "                         probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                         max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=42)\n",
    "\n",
    "# Huấn luyện mô hình SVM\n",
    "svm_rbf_classifier.fit(train_features_mfcc_scaled, train_labels_mfcc)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_svm_rbf = svm_rbf_classifier.predict(val_features_mfcc_scaled)\n",
    "val_accuracy_svm_rbf = accuracy_score(val_labels_mfcc, val_predictions_svm_rbf)\n",
    "print(f\"Validation Accuracy (SVM with RBF Kernel): {val_accuracy_svm_rbf * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_svm_rbf = svm_rbf_classifier.predict(test_features_mfcc_scaled)\n",
    "scale_test_accuracy_mfcc_svm = accuracy_score(test_labels_mfcc, test_predictions_svm_rbf)\n",
    "print(f\"Test Accuracy (SVM with RBF Kernel): {scale_test_accuracy_mfcc_svm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98.55%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "def apply_preprocessing(signal_in, sample_rate):\n",
    "    \"\"\"\n",
    "    Apply optional preprocessing steps on the signal:\n",
    "      - Apply pre-emphasis filtering.\n",
    "    \"\"\"\n",
    "    # Pre-emphasis filter: Boosting higher frequencies slightly\n",
    "    pre_emphasis = 0.97\n",
    "    signal_in = np.append(signal_in[0], signal_in[1:] - pre_emphasis * signal_in[:-1])\n",
    "    return signal_in\n",
    "\n",
    "def frame_signal(signal, frame_size, hop_length):\n",
    "    \"\"\"\n",
    "    Convert a signal into overlapping frames.\n",
    "    \n",
    "    Args:\n",
    "        signal: Input audio signal\n",
    "        frame_size: Number of samples per frame\n",
    "        hop_length: Number of samples between frames\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Framed signal of shape (frame_length, num_frames)\n",
    "    \"\"\"\n",
    "    # Calculate the number of frames\n",
    "    num_frames = 1 + int((len(signal) - frame_size) / hop_length)\n",
    "    \n",
    "    # Create an empty array to store frames\n",
    "    frames = np.zeros((frame_size, num_frames))\n",
    "    \n",
    "    # Frame the signal\n",
    "    for i in range(num_frames):\n",
    "        start = i * hop_length\n",
    "        end = start + frame_size\n",
    "        if end <= len(signal):\n",
    "            frames[:, i] = signal[start:end]\n",
    "        else:  # Zero padding for last frame if needed\n",
    "            frames[:len(signal)-start, i] = signal[start:]\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def apply_window(frames, window_type='hann'):\n",
    "    \"\"\"\n",
    "    Apply a window function to each frame.\n",
    "    \n",
    "    Args:\n",
    "        frames: Framed signal of shape (frame_length, num_frames)\n",
    "        window_type: Type of window function ('hann', 'hamming', 'blackman', etc.)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Windowed frames\n",
    "    \"\"\"\n",
    "    # Create the window function\n",
    "    frame_length = frames.shape[0]\n",
    "    window = np.zeros(frame_length)\n",
    "    \n",
    "    if window_type == 'hann':\n",
    "        window = np.hanning(frame_length)\n",
    "    elif window_type == 'hamming':\n",
    "        window = np.hamming(frame_length)\n",
    "    elif window_type == 'blackman':\n",
    "        window = np.blackman(frame_length)\n",
    "    else:  # Default to Hann window\n",
    "        window = np.hanning(frame_length)\n",
    "    \n",
    "    # Apply window to each frame (element-wise multiplication)\n",
    "    return frames * window.reshape(-1, 1)\n",
    "\n",
    "def compute_stfts_features(signal_in, sample_rate, n_fft=2048, hop_length=512, \n",
    "                          window_type='hann', apply_log=True):\n",
    "    \"\"\"\n",
    "    Compute STFT features with explicitly separated steps:\n",
    "      - Apply preprocessing (pre-emphasis)\n",
    "      - Frame the signal into overlapping segments\n",
    "      - Apply windowing to each frame\n",
    "      - Compute STFT\n",
    "      - Extract features with optional log scaling\n",
    "    \"\"\"\n",
    "    # Apply preprocessing (e.g., pre-emphasis)\n",
    "    signal_in = apply_preprocessing(signal_in, sample_rate)\n",
    "    \n",
    "    # Explicit framing and windowing (for demonstration, though librosa.stft does this internally)\n",
    "    frames = frame_signal(signal_in, n_fft, hop_length)\n",
    "    windowed_frames = apply_window(frames, window_type)\n",
    "    \n",
    "    # Note: In practice, we could use these windowed frames directly for FFT\n",
    "    # but for compatibility with the original code, we'll use librosa.stft\n",
    "    \n",
    "    # Compute STFT with a window function to avoid spectral leakage\n",
    "    stfts_matrix = np.abs(librosa.stft(signal_in, n_fft=n_fft, hop_length=hop_length, window=window_type))\n",
    "    \n",
    "    # Optional: Apply logarithmic scaling (dynamic range compression)\n",
    "    if apply_log:\n",
    "        stfts_matrix = np.log(stfts_matrix + 1e-10)  # More robust epsilon for log scaling\n",
    "\n",
    "    # Feature extraction: Mean and additional energy-based features (e.g., variance)\n",
    "    features_mean = np.mean(stfts_matrix, axis=1)\n",
    "    features_variance = np.var(stfts_matrix, axis=1)\n",
    "    \n",
    "    # Combine mean and variance (you could also add other stats like median or skewness)\n",
    "    features = np.concatenate([features_mean, features_variance], axis=0)\n",
    "\n",
    "    # Normalize the features (optional)\n",
    "    features = (features - np.mean(features)) / np.std(features)  # Z-score normalization\n",
    "\n",
    "    return features\n",
    "\n",
    "def process_audio_file(file_path, n_fft=2048, hop_length=512, window_type='hann'):\n",
    "    \"\"\"\n",
    "    Process an audio file to extract STFT features.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the audio file\n",
    "        n_fft: FFT window size\n",
    "        hop_length: Hop length between frames\n",
    "        window_type: Type of window function to apply\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Extracted features\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    signal, sample_rate = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    # Compute features\n",
    "    features = compute_stfts_features(signal, sample_rate, n_fft, hop_length, window_type)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train data...\n",
      "STFTs extraction time: 241.89 seconds\n",
      "Extracting val data...\n",
      "STFTs extraction time: 34.09 seconds\n",
      "Extracting test data...\n",
      "STFTs extraction time: 68.79 seconds\n"
     ]
    }
   ],
   "source": [
    "def load_stfts_features_from_directory(directory, sample_rate=22050, n_fft=2048, hop_length=512, output_dir=None, dataset_type='train'):\n",
    "    \"\"\"\n",
    "    Load and compute stfts features from audio files in a directory, with improved accuracy.\n",
    "    Assumes 'Queen' and 'NonQueen' subdirectories are present.\n",
    "    \"\"\"\n",
    "    # Kiểm tra nếu file đã tồn tại, thì load lại\n",
    "    features_file = os.path.join(output_dir, f'stfts_features_{dataset_type}1.pkl')\n",
    "    labels_file = os.path.join(output_dir, f'stfts_labels_{dataset_type}1.pkl')\n",
    "    \n",
    "    if os.path.exists(features_file) and os.path.exists(labels_file):\n",
    "        print(f\"Loading {dataset_type}1 data from .pkl files...\")\n",
    "        features = joblib.load(features_file)\n",
    "        labels = joblib.load(labels_file)\n",
    "    else:\n",
    "        print(f\"Extracting {dataset_type} data...\")\n",
    "        start_time = time.time()\n",
    "        labels = []\n",
    "        features = []\n",
    "\n",
    "        for label in ['Queen', 'NonQueen']:\n",
    "            path = os.path.join(directory, label)\n",
    "            for file in os.listdir(path):\n",
    "                file_path = os.path.join(path, file)\n",
    "                # Load audio file\n",
    "                signal, sr = librosa.load(file_path, sr=sample_rate)\n",
    "\n",
    "                # Compute stfts features\n",
    "                stfts_feature = compute_stfts_features(signal, sr, n_fft=n_fft, hop_length=hop_length, apply_log=True)\n",
    "\n",
    "                features.append(stfts_feature)\n",
    "                labels.append(label)\n",
    "\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Lưu dữ liệu vào file .pkl nếu chưa tồn tại\n",
    "        if output_dir:\n",
    "            joblib.dump(features, features_file)\n",
    "            joblib.dump(labels, labels_file)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"STFTs extraction time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "train_features_stfts, train_labels_stfts = load_stfts_features_from_directory(train_path, sample_rate=22050, n_fft=2048, hop_length=512, output_dir=output_dir, dataset_type='train')\n",
    "val_features_stfts, val_labels_stfts = load_stfts_features_from_directory(val_path, sample_rate=22050, n_fft=2048, hop_length=512, output_dir=output_dir, dataset_type='val')\n",
    "test_features_stfts, test_labels_stfts = load_stfts_features_from_directory(test_path, sample_rate=22050, n_fft=2048, hop_length=512, output_dir=output_dir, dataset_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_stfts = StandardScaler()\n",
    "train_features_stfts_scaled = scaler_stfts.fit_transform(train_features_stfts)\n",
    "val_features_stfts_scaled   = scaler_stfts.transform(val_features_stfts)\n",
    "test_features_stfts_scaled  = scaler_stfts.transform(test_features_stfts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STFTs - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.00 seconds\n",
      "KNN (STFTs) - Test Accuracy: 93.30%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "knn_classifier_stfts = KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                           metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "knn_classifier_stfts.fit(train_features_stfts, train_labels_stfts)\n",
    "val_pred_stfts_knn = knn_classifier_stfts.predict(val_features_stfts)\n",
    "test_pred_stfts_knn = knn_classifier_stfts.predict(test_features_stfts)\n",
    "\n",
    "test_accuracy_stfts_knn = accuracy_score(test_labels_stfts, test_pred_stfts_knn)\n",
    "print(f\"KNN (STFTs) - Test Accuracy: {test_accuracy_stfts_knn*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old 95.10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.00 seconds\n",
      "KNN (STFTs) - Test Accuracy: 92.10%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "knn_classifier_stfts = KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                           metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "knn_classifier_stfts.fit(train_features_stfts_scaled, train_labels_stfts)\n",
    "val_pred_stfts_knn = knn_classifier_stfts.predict(val_features_stfts_scaled)\n",
    "test_pred_stfts_knn = knn_classifier_stfts.predict(test_features_stfts_scaled)\n",
    "\n",
    "test_accuracy_stfts_knn = accuracy_score(test_labels_stfts, test_pred_stfts_knn)\n",
    "print(f\"KNN (STFTs) - Test Accuracy: {test_accuracy_stfts_knn*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old 94.50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STFTs - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 245.16 seconds\n",
      "Validation Accuracy (SVM with RBF Kernel): 96.32%\n",
      "Test Accuracy (SVM with RBF Kernel): 95.58%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "svm_rbf_classifier = SVC(C=10, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, \n",
    "                         probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                         max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=42)\n",
    "\n",
    "# Huấn luyện mô hình SVM\n",
    "svm_rbf_classifier.fit(train_features_stfts_scaled, train_labels_stfts)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_svm_rbf = svm_rbf_classifier.predict(val_features_stfts_scaled)\n",
    "val_accuracy_stfts_svm = accuracy_score(val_labels_stfts, val_predictions_svm_rbf)\n",
    "print(f\"Validation Accuracy (SVM with RBF Kernel): {val_accuracy_stfts_svm * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_svm_rbf = svm_rbf_classifier.predict(test_features_stfts_scaled)\n",
    "scale_test_accuracy_stfts_svm = accuracy_score(test_labels_stfts, test_predictions_svm_rbf)\n",
    "print(f\"Test Accuracy (SVM with RBF Kernel): {scale_test_accuracy_stfts_svm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STFTs - LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 37.37 seconds\n",
      "Logistic Regression (STFTs) - Test Accuracy: 88.65%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "lr_classifier_stfts = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear', penalty= 'l2', C= 0.08858667904100823)\n",
    "lr_classifier_stfts.fit(train_features_stfts_scaled, train_labels_stfts)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_stfts_lr = lr_classifier_stfts.predict(val_features_stfts_scaled)\n",
    "test_pred_stfts_lr = lr_classifier_stfts.predict(test_features_stfts_scaled)\n",
    "\n",
    "test_accuracy_stfts_lr = accuracy_score(test_labels_stfts, test_pred_stfts_lr)\n",
    "print(f\"Logistic Regression (STFTs) - Test Accuracy: {test_accuracy_stfts_lr*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STFTs - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 169.32 seconds\n",
      "Random Forest (STFTs) - Test Accuracy: 93.30%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "rf_classifier_stfts = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, \n",
    "                                       min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                       max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                       bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, \n",
    "                                       warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "rf_classifier_stfts.fit(train_features_stfts, train_labels_stfts)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_stfts_rf = rf_classifier_stfts.predict(val_features_stfts)\n",
    "test_pred_stfts_rf = rf_classifier_stfts.predict(test_features_stfts)\n",
    "\n",
    "test_accuracy_stfts_rf = accuracy_score(test_labels_stfts, test_pred_stfts_rf)\n",
    "print(f\"Random Forest (STFTs) - Test Accuracy: {test_accuracy_stfts_rf*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STFTs - ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 180.13 seconds\n",
      "Extra Trees (stfts) - Test Accuracy: 91.35%\n"
     ]
    }
   ],
   "source": [
    "start_time\n",
    "\n",
    "et_classifier_stfts = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "et_classifier_stfts.fit(train_features_stfts_scaled, train_labels_stfts)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_stfts_et = et_classifier_stfts.predict(val_features_stfts_scaled)\n",
    "test_pred_stfts_et = et_classifier_stfts.predict(test_features_stfts_scaled)\n",
    "\n",
    "test_accuracy_stfts_et = accuracy_score(test_labels_stfts, test_pred_stfts_et)\n",
    "print(f\"Extra Trees (STFTs) - Test Accuracy: {test_accuracy_stfts_et*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CQT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cqt_features(signal_in, sample_rate, hop_length=512, fmin=None, \n",
    "                         n_bins=84, bins_per_octave=12, apply_log=True):\n",
    "    \"\"\"\n",
    "    Tính đặc trưng CQT của tín hiệu:\n",
    "      - Sử dụng librosa.cqt để tính ma trận CQT, lấy giá trị magnitude.\n",
    "      - (Tùy chọn) Áp dụng log scaling để giảm phạm vi giá trị.\n",
    "      - Trung bình theo trục thời gian (axis=1) để thu được vector đặc trưng có kích thước (n_bins,).\n",
    "      \n",
    "    Nếu fmin không được chỉ định, librosa sẽ tự động sử dụng fmin mặc định.\n",
    "    \"\"\"\n",
    "    cqt_matrix = np.abs(librosa.cqt(signal_in, sr=sample_rate, hop_length=hop_length,\n",
    "                                    fmin=fmin, n_bins=n_bins, bins_per_octave=bins_per_octave))\n",
    "    if apply_log:\n",
    "        cqt_matrix = np.log(cqt_matrix + 1e-8)\n",
    "    # Trung bình theo trục thời gian (tính trung bình các cột)\n",
    "    features = np.mean(cqt_matrix, axis=1)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train data...\n",
      "CQT extraction time: 591.23 seconds\n",
      "Extracting val data...\n",
      "CQT extraction time: 85.39 seconds\n",
      "Extracting test data...\n",
      "CQT extraction time: 172.54 seconds\n"
     ]
    }
   ],
   "source": [
    "def load_cqt_features_from_directory(directory, sample_rate=22050, hop_length=512, \n",
    "                                     fmin=None, n_bins=84, bins_per_octave=12, output_dir=None, dataset_type='train'):\n",
    "    # Kiểm tra nếu file đã tồn tại, thì load lại\n",
    "    features_file = os.path.join(output_dir, f'cqt_features_{dataset_type}.pkl')\n",
    "    labels_file = os.path.join(output_dir, f'cqt_labels_{dataset_type}.pkl')\n",
    "    \n",
    "    if os.path.exists(features_file) and os.path.exists(labels_file):\n",
    "        print(f\"Loading {dataset_type} data from .pkl files...\")\n",
    "        features = joblib.load(features_file)\n",
    "        labels = joblib.load(labels_file)\n",
    "    else:\n",
    "        print(f\"Extracting {dataset_type} data...\")\n",
    "        start_time = time.time()\n",
    "        labels = []\n",
    "        features = []\n",
    "        \n",
    "        for label in ['Queen', 'NonQueen']:\n",
    "            path = os.path.join(directory, label)\n",
    "            for file in os.listdir(path):\n",
    "                file_path = os.path.join(path, file)\n",
    "                signal, sr = librosa.load(file_path, sr=sample_rate)\n",
    "                \n",
    "                # Compute CQT features\n",
    "                cqt_feature = compute_cqt_features(signal, sr, hop_length=hop_length, \n",
    "                                                   fmin=fmin, n_bins=n_bins, bins_per_octave=bins_per_octave, \n",
    "                                                   apply_log=True)\n",
    "                \n",
    "                features.append(cqt_feature)\n",
    "                labels.append(label)\n",
    "\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Lưu dữ liệu vào file .pkl nếu chưa tồn tại\n",
    "        if output_dir:\n",
    "            joblib.dump(features, features_file)\n",
    "            joblib.dump(labels, labels_file)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"CQT extraction time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "train_features_cqt, train_labels_cqt = load_cqt_features_from_directory(train_path, sample_rate=22050,\n",
    "                                                                        hop_length=512, fmin=None, n_bins=84, bins_per_octave=12, \n",
    "                                                                        output_dir=output_dir, dataset_type='train')\n",
    "val_features_cqt, val_labels_cqt = load_cqt_features_from_directory(val_path, sample_rate=22050,\n",
    "                                                                    hop_length=512, fmin=None, n_bins=84, bins_per_octave=12, \n",
    "                                                                    output_dir=output_dir, dataset_type='val')\n",
    "test_features_cqt, test_labels_cqt = load_cqt_features_from_directory(test_path, sample_rate=22050,\n",
    "                                                                      hop_length=512, fmin=None, n_bins=84, bins_per_octave=12, \n",
    "                                                                      output_dir=output_dir, dataset_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_cqt = StandardScaler()\n",
    "train_features_cqt_scaled = scaler_cqt.fit_transform(train_features_cqt)\n",
    "val_features_cqt_scaled   = scaler_cqt.transform(val_features_cqt)\n",
    "test_features_cqt_scaled  = scaler_cqt.transform(test_features_cqt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CQT - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.01 seconds\n",
      "KNN (CQT) - Test Accuracy: 97.52%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "knn_classifier_cqt = KNeighborsClassifier(n_neighbors=1, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                      metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "knn_classifier_cqt.fit(train_features_cqt_scaled, train_labels_cqt)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_cqt_knn = knn_classifier_cqt.predict(val_features_cqt_scaled)\n",
    "test_pred_cqt_knn = knn_classifier_cqt.predict(test_features_cqt_scaled)\n",
    "\n",
    "test_accuracy_cqt_knn = accuracy_score(test_labels_cqt, test_pred_cqt_knn)\n",
    "print(f\"KNN (CQT) - Test Accuracy: {test_accuracy_cqt_knn*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CQT - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 7.56 seconds\n",
      "SVM (CQT) - Test Accuracy: 96.85%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "svm_classifier_cqt = SVC(C=100, kernel='rbf', gamma='auto', random_state=42)\n",
    "svm_classifier_cqt.fit(train_features_cqt_scaled, train_labels_cqt)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_cqt_svm = svm_classifier_cqt.predict(val_features_cqt_scaled)\n",
    "test_pred_cqt_svm = svm_classifier_cqt.predict(test_features_cqt_scaled)\n",
    "\n",
    "test_accuracy_cqt_svm = accuracy_score(test_labels_cqt, test_pred_cqt_svm)\n",
    "print(f\"SVM (CQT) - Test Accuracy: {test_accuracy_cqt_svm*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CQT - LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.59 seconds\n",
      "Logistic Regression (CQT) - Test Accuracy: 83.60%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "lr_classifier_cqt =LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, \n",
    "                                      fit_intercept=True, intercept_scaling=1, \n",
    "                                      class_weight=None, random_state=None, solver='lbfgs', \n",
    "                                      max_iter=500, multi_class='deprecated', verbose=0, \n",
    "                                      warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "lr_classifier_cqt.fit(train_features_cqt_scaled, train_labels_cqt)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_cqt_lr = lr_classifier_cqt.predict(val_features_cqt_scaled)\n",
    "test_pred_cqt_lr = lr_classifier_cqt.predict(test_features_cqt_scaled)\n",
    "\n",
    "test_accuracy_cqt_lr = accuracy_score(test_labels_cqt, test_pred_cqt_lr)\n",
    "print(f\"Logistic Regression (CQT) - Test Accuracy: {test_accuracy_cqt_lr*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CQT - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 29.00 seconds\n",
      "Random Forest (CQT) - Test Accuracy: 95.45%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "rf_classifier_cqt = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, \n",
    "                                       min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                       max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                       bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, \n",
    "                                       warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "rf_classifier_cqt.fit(train_features_cqt, train_labels_cqt)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_cqt_rf = rf_classifier_cqt.predict(val_features_cqt)\n",
    "test_pred_cqt_rf = rf_classifier_cqt.predict(test_features_cqt)\n",
    "\n",
    "test_accuracy_cqt_rf = accuracy_score(test_labels_cqt, test_pred_cqt_rf)\n",
    "print(f\"Random Forest (CQT) - Test Accuracy: {test_accuracy_cqt_rf*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CQT - ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 30.03 seconds\n",
      "Extra Trees (CQT) - Test Accuracy: 95.53%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "et_classifier_cqt = ExtraTreesClassifier(n_estimators=200, criterion='gini', max_depth=30, min_samples_split=2, min_samples_leaf=1, \n",
    "                                     min_weight_fraction_leaf=0.0, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                     bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, warm_start=False, \n",
    "                                     class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "et_classifier_cqt.fit(train_features_cqt_scaled, train_labels_cqt)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_cqt_et = et_classifier_cqt.predict(val_features_cqt_scaled)\n",
    "test_pred_cqt_et = et_classifier_cqt.predict(test_features_cqt_scaled)\n",
    "\n",
    "test_accuracy_cqt_et = accuracy_score(test_labels_cqt, test_pred_cqt_et)\n",
    "print(f\"Extra Trees (CQT) - Test Accuracy: {test_accuracy_cqt_et*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma + Sprectral centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chroma_and_centroid_features(signal_in, sample_rate, n_fft=2048, hop_length=512, n_chroma=12):\n",
    "    \"\"\"\n",
    "    Tính cả đặc trưng Chroma và Spectral Centroid của tín hiệu:\n",
    "      - Sử dụng librosa.feature.chroma_stft để tính ma trận chroma.\n",
    "      - Sử dụng librosa.feature.spectral_centroid để tính Spectral Centroid.\n",
    "      - Trung bình theo trục thời gian (axis=1) để thu được vector đặc trưng cho Chroma.\n",
    "      - Trung bình theo trục thời gian (axis=1) để thu được vector đặc trưng cho Spectral Centroid.\n",
    "      - Trả về vector đặc trưng kết hợp (chroma + centroid).\n",
    "    \"\"\"\n",
    "    # Tính đặc trưng Chroma\n",
    "    chroma = librosa.feature.chroma_stft(y=signal_in, sr=sample_rate, n_fft=n_fft, \n",
    "                                           hop_length=hop_length, n_chroma=n_chroma)\n",
    "    chroma_feature = np.mean(chroma, axis=1)\n",
    "\n",
    "    # Tính Spectral Centroid\n",
    "    centroid = librosa.feature.spectral_centroid(y=signal_in, sr=sample_rate, n_fft=n_fft, \n",
    "                                                 hop_length=hop_length)\n",
    "    centroid_feature = np.mean(centroid, axis=1)\n",
    "\n",
    "    # Kết hợp các đặc trưng vào một vector duy nhất\n",
    "    combined_feature = np.concatenate((chroma_feature, centroid_feature), axis=0)\n",
    "    return combined_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train data...\n",
      "Combined features extraction time: 197.54 seconds\n",
      "Extracting val data...\n",
      "Combined features extraction time: 28.23 seconds\n",
      "Extracting test data...\n",
      "Combined features extraction time: 56.15 seconds\n"
     ]
    }
   ],
   "source": [
    "def load_combined_features_from_directory(directory, sample_rate=22050, n_fft=2048, hop_length=512, n_chroma=12, output_dir=None, dataset_type='train'):\n",
    "    # Kiểm tra nếu file đã tồn tại, thì load lại\n",
    "    features_file = os.path.join(output_dir, f'combined_features_{dataset_type}.pkl')\n",
    "    labels_file = os.path.join(output_dir, f'combined_labels_{dataset_type}.pkl')\n",
    "    \n",
    "    if os.path.exists(features_file) and os.path.exists(labels_file):\n",
    "        print(f\"Loading {dataset_type} data from .pkl files...\")\n",
    "        features = joblib.load(features_file)\n",
    "        labels = joblib.load(labels_file)\n",
    "    else:\n",
    "        print(f\"Extracting {dataset_type} data...\")\n",
    "        start_time = time.time()\n",
    "        labels = []\n",
    "        features = []\n",
    "\n",
    "        for label in ['Queen', 'NonQueen']:\n",
    "            path = os.path.join(directory, label)\n",
    "            for file in os.listdir(path):\n",
    "                file_path = os.path.join(path, file)\n",
    "                signal, sr = librosa.load(file_path, sr=sample_rate)\n",
    "                \n",
    "                # Tính toán các đặc trưng kết hợp (chroma + centroid)\n",
    "                combined_feature = compute_chroma_and_centroid_features(signal, sr, n_fft=n_fft, \n",
    "                                                                        hop_length=hop_length, n_chroma=n_chroma)\n",
    "                \n",
    "                features.append(combined_feature)\n",
    "                labels.append(label)\n",
    "\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Lưu dữ liệu vào file .pkl nếu chưa tồn tại\n",
    "        if output_dir:\n",
    "            joblib.dump(features, features_file)\n",
    "            joblib.dump(labels, labels_file)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Combined features extraction time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "train_features_combined, train_labels_combined = load_combined_features_from_directory(train_path, sample_rate=22050, n_fft=2048, hop_length=512, n_chroma=12, output_dir=output_dir, dataset_type='train')\n",
    "val_features_combined, val_labels_combined = load_combined_features_from_directory(val_path, sample_rate=22050, n_fft=2048, hop_length=512, n_chroma=12, output_dir=output_dir, dataset_type='val')\n",
    "test_features_combined, test_labels_combined = load_combined_features_from_directory(test_path, sample_rate=22050, n_fft=2048, hop_length=512, n_chroma=12, output_dir=output_dir, dataset_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_combined = StandardScaler()\n",
    "train_features_combined_scaled = scaler_combined.fit_transform(train_features_combined)\n",
    "val_features_combined_scaled   = scaler_combined.transform(val_features_combined)\n",
    "test_features_combined_scaled  = scaler_combined.transform(test_features_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma + SC - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.03 seconds\n",
      "KNN (combined) - Test Accuracy: 79.53%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "knn_classifier_combined = KNeighborsClassifier(n_neighbors=9, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                      metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "knn_classifier_combined.fit(train_features_combined_scaled, train_labels_combined)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_combined_knn = knn_classifier_combined.predict(val_features_combined_scaled)\n",
    "test_pred_combined_knn = knn_classifier_combined.predict(test_features_combined_scaled)\n",
    "\n",
    "test_accuracy_combined_knn = accuracy_score(test_labels_combined, test_pred_combined_knn)\n",
    "print(f\"KNN (combined) - Test Accuracy: {test_accuracy_combined_knn*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma + SC - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 14.54 seconds\n",
      "SVM (combined) - Test Accuracy: 81.47%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "svm_classifier_combined = SVC(C=50, kernel='rbf', gamma='auto', random_state=42)\n",
    "svm_classifier_combined.fit(train_features_combined_scaled, train_labels_combined)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_combined_svm = svm_classifier_combined.predict(val_features_combined_scaled)\n",
    "test_pred_combined_svm = svm_classifier_combined.predict(test_features_combined_scaled)\n",
    "\n",
    "test_accuracy_combined_svm = accuracy_score(test_labels_combined, test_pred_combined_svm)\n",
    "print(f\"SVM (combined) - Test Accuracy: {test_accuracy_combined_svm*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma + SC - LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.06 seconds\n",
      "Logistic Regression (combined) - Test Accuracy: 62.62%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "lr_classifier_combined =LogisticRegression(penalty='l2', dual=False, tol=0.001, C=1, \n",
    "                                      fit_intercept=True, intercept_scaling=1, \n",
    "                                      class_weight=None, random_state=None, solver='lbfgs', \n",
    "                                      max_iter=500, multi_class='deprecated', verbose=0, \n",
    "                                      warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "lr_classifier_combined.fit(train_features_combined_scaled, train_labels_combined)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_combined_lr = lr_classifier_combined.predict(val_features_combined_scaled)\n",
    "test_pred_combined_lr = lr_classifier_combined.predict(test_features_combined_scaled)\n",
    "\n",
    "test_accuracy_combined_lr = accuracy_score(test_labels_combined, test_pred_combined_lr)\n",
    "print(f\"Logistic Regression (combined) - Test Accuracy: {test_accuracy_combined_lr*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma + SC - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 10.30 seconds\n",
      "Random Forest (combined) - Test Accuracy: 81.27%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "rf_classifier_combined = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, \n",
    "                                       min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                       max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                       bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, \n",
    "                                       warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "rf_classifier_combined.fit(train_features_combined, train_labels_combined)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_combined_rf = rf_classifier_combined.predict(val_features_combined)\n",
    "test_pred_combined_rf = rf_classifier_combined.predict(test_features_combined)\n",
    "\n",
    "test_accuracy_combined_rf = accuracy_score(test_labels_combined, test_pred_combined_rf)\n",
    "print(f\"Random Forest (combined) - Test Accuracy: {test_accuracy_combined_rf*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma + SC - ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 10.98 seconds\n",
      "Extra Trees (combined) - Test Accuracy: 81.90%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "et_classifier_combined = ExtraTreesClassifier(n_estimators=300, criterion='gini', max_depth=30, min_samples_split=2, min_samples_leaf=1, \n",
    "                                     min_weight_fraction_leaf=0.0, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                     bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, warm_start=False, \n",
    "                                     class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "et_classifier_combined.fit(train_features_combined_scaled, train_labels_combined)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_combined_et = et_classifier_combined.predict(val_features_combined_scaled)\n",
    "test_pred_combined_et = et_classifier_combined.predict(test_features_combined_scaled)\n",
    "\n",
    "test_accuracy_combined_et = accuracy_score(test_labels_combined, test_pred_combined_et)\n",
    "print(f\"Extra Trees (combined) - Test Accuracy: {test_accuracy_combined_et*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chroma_energy(signal_in, sample_rate, frame_size=0.025, frame_stride=0.01):\n",
    "    stft = librosa.stft(signal_in, n_fft=int(frame_size * sample_rate), hop_length=int(frame_stride * sample_rate))\n",
    "    chroma = librosa.feature.chroma_stft(S=np.abs(stft), sr=sample_rate)\n",
    "    chroma_energy = np.sum(chroma**2, axis=1)\n",
    "    return chroma_energy\n",
    "\n",
    "def compute_rmse(signal_in, sample_rate=22050, frame_size=0.025, frame_stride=0.01):\n",
    "    rmse = librosa.feature.rms(y=signal_in)\n",
    "    return rmse[0]\n",
    "\n",
    "def compute_zcr(signal_in, sample_rate=22050, frame_size=0.025, frame_stride=0.01):\n",
    "    zcr = librosa.feature.zero_crossing_rate(signal_in, frame_length=int(frame_size * sample_rate), hop_length=int(frame_stride * sample_rate))\n",
    "    return zcr[0]\n",
    "\n",
    "def compute_spectral_flux(signal_in, sample_rate=22050, frame_size=0.025, frame_stride=0.01):\n",
    "    stft = librosa.stft(signal_in, n_fft=int(frame_size * sample_rate), hop_length=int(frame_stride * sample_rate))\n",
    "    mag_frames = np.abs(stft)\n",
    "    flux = np.diff(mag_frames, axis=1)\n",
    "    spectral_flux = np.sum(flux, axis=0)\n",
    "    return spectral_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from .pkl files...\n",
      "Loading val data from .pkl files...\n",
      "Loading test data from .pkl files...\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_rate=22050, output_dir=None, dataset_type='train'):\n",
    "    # Kiểm tra nếu file đã tồn tại, thì load lại\n",
    "    features_file = os.path.join(output_dir, f'features_{dataset_type}.pkl')\n",
    "    labels_file = os.path.join(output_dir, f'labels_{dataset_type}.pkl')\n",
    "    data_file = os.path.join(output_dir, f'data_{dataset_type}.pkl')\n",
    "    \n",
    "    if os.path.exists(features_file) and os.path.exists(labels_file) and os.path.exists(data_file):\n",
    "        print(f\"Loading {dataset_type} data from .pkl files...\")\n",
    "        features = joblib.load(features_file)\n",
    "        labels = joblib.load(labels_file)\n",
    "        data = joblib.load(data_file)\n",
    "    else:\n",
    "        print(f\"Extracting {dataset_type} data...\")\n",
    "        start_time = time.time()\n",
    "        labels = []\n",
    "        features = []\n",
    "        data = []  # Thêm một biến để lưu dữ liệu đầu vào (có thể là tín hiệu, ví dụ như signal hoặc các thông tin khác)\n",
    "        \n",
    "        for label in ['Queen', 'NonQueen']:\n",
    "            path = os.path.join(directory, label)\n",
    "            for file in os.listdir(path):\n",
    "                file_path = os.path.join(path, file)\n",
    "                signal, sr = librosa.load(file_path, sr=sample_rate)\n",
    "                \n",
    "                # Tính toán các đặc trưng\n",
    "                chroma_energy = compute_chroma_energy(signal, sr)\n",
    "                rmse = compute_rmse(signal, sr)\n",
    "                zcr = compute_zcr(signal, sr)\n",
    "                spectral_flux = compute_spectral_flux(signal, sr)\n",
    "                \n",
    "                # Tính trung bình các đặc trưng nếu cần\n",
    "                feature = np.hstack([chroma_energy, rmse, zcr, spectral_flux])\n",
    "                \n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "                data.append(signal)  # Lưu tín hiệu âm thanh gốc hoặc dữ liệu khác nếu cần\n",
    "        \n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        data = np.array(data)  # Chuyển dữ liệu sang dạng numpy array nếu cần lưu\n",
    "        \n",
    "        if output_dir:\n",
    "            # Lưu từng đối tượng riêng biệt cho train/val/test\n",
    "            joblib.dump(features, features_file)\n",
    "            joblib.dump(labels, labels_file)\n",
    "            joblib.dump(data, data_file)  # Lưu tín hiệu âm thanh hoặc dữ liệu\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Feature extraction time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return features, labels, data\n",
    "\n",
    "train_features, train_labels, train_data = extract_features(train_path, output_dir=output_dir, dataset_type='train')\n",
    "val_features, val_labels, val_data = extract_features(val_path, output_dir=output_dir, dataset_type='val')\n",
    "test_features, test_labels, test_data = extract_features(test_path, output_dir=output_dir, dataset_type='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_cafe = StandardScaler()\n",
    "train_features_cafe_scaled = scaler_cafe.fit_transform(train_features)\n",
    "val_features_cafe_scaled   = scaler_cafe.transform(val_features)\n",
    "test_features_cafe_scaled  = scaler_cafe.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.02 seconds\n",
      "KNN (cafe) - Test Accuracy: 63.45%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "knn_classifier_cafe = KNeighborsClassifier(n_neighbors=13, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                      metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "knn_classifier_cafe.fit(train_features_cafe_scaled, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_cafe_knn = knn_classifier_cafe.predict(val_features_cafe_scaled)\n",
    "test_pred_cafe_knn = knn_classifier_cafe.predict(test_features_cafe_scaled)\n",
    "\n",
    "test_accuracy_cafe_knn = accuracy_score(test_labels, test_pred_cafe_knn)\n",
    "print(f\"KNN (cafe) - Test Accuracy: {test_accuracy_cafe_knn*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 273.19 seconds\n",
      "SVM (cafe) - Test Accuracy: 76.53%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "svm_classifier_cafe = SVC(C=10, kernel='rbf', gamma='auto', random_state=42)\n",
    "svm_classifier_cafe.fit(train_features_cafe_scaled, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_cafe_svm = svm_classifier_cafe.predict(val_features_cafe_scaled)\n",
    "test_pred_cafe_svm = svm_classifier_cafe.predict(test_features_cafe_scaled)\n",
    "\n",
    "test_accuracy_cafe_svm = accuracy_score(test_labels, test_pred_cafe_svm)\n",
    "print(f\"SVM (cafe) - Test Accuracy: {test_accuracy_cafe_svm*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFT + CQT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train data...\n",
      "Feature extraction time: 685.76 seconds\n",
      "Features shape: (14066, 353), Labels shape: (14066,)\n",
      "Extracting val data...\n",
      "Feature extraction time: 97.72 seconds\n",
      "Features shape: (2010, 353), Labels shape: (2010,)\n",
      "Extracting test data...\n",
      "Feature extraction time: 192.34 seconds\n",
      "Features shape: (4000, 353), Labels shape: (4000,)\n"
     ]
    }
   ],
   "source": [
    "def pre_emphasis(signal_in, pre_emph=0.97):\n",
    "    \"\"\"\n",
    "    Áp dụng pre-emphasis để nhấn mạnh các tần số cao.\n",
    "    \n",
    "    Args:\n",
    "        signal_in: Tín hiệu đầu vào\n",
    "        pre_emph: Hệ số pre-emphasis, mặc định là 0.97\n",
    "        \n",
    "    Returns:\n",
    "        Tín hiệu sau khi áp dụng pre-emphasis\n",
    "    \"\"\"\n",
    "    return np.append(signal_in[0], signal_in[1:] - pre_emph * signal_in[:-1])\n",
    "\n",
    "def framing(signal_in, sample_rate, frame_size=0.025, frame_stride=0.01):\n",
    "    \"\"\"\n",
    "    Chia tín hiệu thành các frame có kích thước và bước nhảy xác định.\n",
    "    \"\"\"\n",
    "    frame_length = int(round(frame_size * sample_rate))\n",
    "    frame_step = int(round(frame_stride * sample_rate))\n",
    "    signal_length = len(signal_in)\n",
    "    num_frames = int(np.ceil(np.abs(signal_length - frame_length) / frame_step)) + 1\n",
    "\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(signal_in, z)\n",
    "\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + \\\n",
    "              np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    return frames\n",
    "\n",
    "def windowing(frames):\n",
    "    \"\"\"\n",
    "    Áp dụng cửa sổ Hamming cho mỗi frame.\n",
    "    \"\"\"\n",
    "    frame_length = frames.shape[1]\n",
    "    hamming = np.hamming(frame_length)\n",
    "    return frames * hamming\n",
    "\n",
    "def fft_frames(frames, NFFT=512):\n",
    "    \"\"\"\n",
    "    Tính FFT cho mỗi frame và lấy giá trị magnitude.\n",
    "    \"\"\"\n",
    "    return np.absolute(np.fft.rfft(frames, NFFT))\n",
    "\n",
    "def compute_fft_features(signal_in, sample_rate, frame_size=0.025, frame_stride=0.01, NFFT=512, apply_log=True):\n",
    "    \"\"\"\n",
    "    Tính toán đặc trưng FFT cho tín hiệu âm thanh:\n",
    "      - Pre-emphasis, Framing, Windowing.\n",
    "      - Tính FFT cho từng frame và lấy giá trị magnitude.\n",
    "      - Trung bình các frame để có vector đặc trưng ổn định.\n",
    "      - (Tùy chọn) Áp dụng log để giảm phạm vi giá trị.\n",
    "      \n",
    "    Trả về: vector đặc trưng có kích thước (NFFT/2+1,).\n",
    "    \"\"\"\n",
    "    emphasized_signal = pre_emphasis(signal_in)\n",
    "    frames = framing(emphasized_signal, sample_rate, frame_size, frame_stride)\n",
    "    windowed_frames = windowing(frames)\n",
    "    mag_frames = fft_frames(windowed_frames, NFFT)\n",
    "    fft_feature = np.mean(mag_frames, axis=0)  # Trung bình theo các frame\n",
    "    if apply_log:\n",
    "        fft_feature = np.log(fft_feature + 1e-8)  # Thêm epsilon để tránh log(0)\n",
    "    return fft_feature\n",
    "\n",
    "def compute_cqt_features(signal_in, sample_rate, fmin=None, \n",
    "                         n_bins=84, bins_per_octave=12, apply_log=True):\n",
    "    \"\"\"\n",
    "    Tính đặc trưng CQT của tín hiệu:\n",
    "      - Sử dụng librosa.cqt để tính ma trận CQT, lấy giá trị magnitude.\n",
    "      - (Tùy chọn) Áp dụng log scaling để giảm phạm vi giá trị.\n",
    "      - Trung bình theo trục thời gian (axis=1) để thu được vector đặc trưng có kích thước (n_bins,).\n",
    "      \n",
    "    Args:\n",
    "        signal_in: Tín hiệu đầu vào\n",
    "        sample_rate: Tần số lấy mẫu\n",
    "        fmin: Tần số thấp nhất (Hz). Nếu None, librosa sẽ sử dụng mặc định\n",
    "        n_bins: Số lượng bin tần số\n",
    "        bins_per_octave: Số lượng bin cho mỗi quãng 8\n",
    "        apply_log: Áp dụng logarit cho đặc trưng\n",
    "        \n",
    "    Returns:\n",
    "        Vector đặc trưng CQT có kích thước (n_bins,)\n",
    "    \"\"\"\n",
    "    cqt_matrix = np.abs(librosa.cqt(signal_in, sr=sample_rate,\n",
    "                                    fmin=fmin, n_bins=n_bins, bins_per_octave=bins_per_octave))\n",
    "    \n",
    "    # Trung bình theo trục thời gian (tính trung bình các cột)\n",
    "    cqt_feature = np.mean(cqt_matrix, axis=1)\n",
    "    \n",
    "    # Áp dụng logarithm (nếu cần)\n",
    "    if apply_log:\n",
    "        cqt_feature = np.log(cqt_feature + 1e-8)  # Thêm epsilon để tránh log(0)\n",
    "    \n",
    "    return cqt_feature\n",
    "\n",
    "def compute_features(signal_in, sample_rate, frame_size=0.025,\n",
    "                     NFFT=512,\n",
    "                     fmin=None, n_bins=84, bins_per_octave=12, apply_log=True):\n",
    "    \"\"\"\n",
    "    Tính toán và kết hợp đặc trưng FFT và CQT cho tín hiệu âm thanh.\n",
    "    \n",
    "    Args:\n",
    "        signal_in: Tín hiệu đầu vào\n",
    "        sample_rate: Tần số lấy mẫu\n",
    "        frame_size: Kích thước frame cho FFT (giây hoặc số mẫu nếu > 1)\n",
    "        NFFT: Số điểm FFT\n",
    "        fmin: Tần số thấp nhất (Hz) cho CQT\n",
    "        n_bins: Số lượng bin tần số cho CQT\n",
    "        bins_per_octave: Số lượng bin cho mỗi quãng 8 cho CQT\n",
    "        apply_log: Áp dụng logarit cho đặc trưng\n",
    "        \n",
    "    Returns:\n",
    "        Vector đặc trưng kết hợp FFT và CQT\n",
    "    \"\"\"\n",
    "    # Tính đặc trưng FFT\n",
    "    fft_feature = compute_fft_features(\n",
    "        signal_in, sample_rate, frame_size=frame_size,\n",
    "        NFFT=NFFT,\n",
    "        apply_log=apply_log\n",
    "    )\n",
    "    \n",
    "    # Tính đặc trưng CQT\n",
    "    cqt_feature = compute_cqt_features(\n",
    "        signal_in, sample_rate, fmin=fmin,\n",
    "        n_bins=n_bins, bins_per_octave=bins_per_octave,\n",
    "        apply_log=apply_log\n",
    "    )\n",
    "    \n",
    "    # Kết hợp hai đặc trưng\n",
    "    combined_feature = np.concatenate((fft_feature, cqt_feature))\n",
    "    \n",
    "    return combined_feature\n",
    "\n",
    "def load_features_from_directory(directory, sample_rate=22050, NFFT=512, \n",
    "                                 frame_size=0.025,\n",
    "                                 output_dir=None, \n",
    "                                 dataset_type='train',\n",
    "                                 fmin=None, n_bins=84, bins_per_octave=12):\n",
    "    \"\"\"\n",
    "    Duyệt qua các file âm thanh trong thư mục và tính đặc trưng kết hợp FFT và CQT cho mỗi file.\n",
    "    \n",
    "    Args:\n",
    "        directory: Thư mục chứa dữ liệu âm thanh\n",
    "        sample_rate: Tần số lấy mẫu\n",
    "        NFFT: Số điểm FFT\n",
    "        frame_size: Kích thước frame (giây hoặc số mẫu nếu > 1)\n",
    "        output_dir: Thư mục đầu ra để lưu đặc trưng\n",
    "        dataset_type: Loại tập dữ liệu ('train', 'test', 'val')\n",
    "        fmin: Tần số thấp nhất (Hz) cho CQT\n",
    "        n_bins: Số lượng bin tần số cho CQT\n",
    "        bins_per_octave: Số lượng bin cho mỗi quãng 8 cho CQT\n",
    "        \n",
    "    Returns:\n",
    "        features: Mảng đặc trưng kết hợp\n",
    "        labels: Nhãn tương ứng\n",
    "    \"\"\"\n",
    "    # Kiểm tra nếu file đã tồn tại, thì load lại\n",
    "    features_file = os.path.join(output_dir, f'combined_features2_{dataset_type}_nfft{NFFT}_bins{n_bins}.pkl')\n",
    "    labels_file = os.path.join(output_dir, f'combined_labels2_{dataset_type}_nfft{NFFT}_bins{n_bins}.pkl')\n",
    "    \n",
    "    if os.path.exists(features_file) and os.path.exists(labels_file):\n",
    "        print(f\"Loading {dataset_type} data from .pkl files...\")\n",
    "        features = joblib.load(features_file)\n",
    "        labels = joblib.load(labels_file)\n",
    "    else:\n",
    "        print(f\"Extracting {dataset_type} data...\")\n",
    "        start_time = time.time()\n",
    "        labels = []\n",
    "        features = []\n",
    "        \n",
    "        for label in ['NonQueen', 'Queen']:\n",
    "            path = os.path.join(directory, label)\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Warning: Path {path} does not exist. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            for file in os.listdir(path):\n",
    "                if not file.endswith(('.wav', '.mp3', '.flac', '.ogg')):\n",
    "                    continue\n",
    "                    \n",
    "                file_path = os.path.join(path, file)\n",
    "                try:\n",
    "                    signal, sr = librosa.load(file_path, sr=sample_rate)\n",
    "                    \n",
    "                    # Tính toán đặc trưng kết hợp FFT và CQT\n",
    "                    combined_feature = compute_features(\n",
    "                        signal, sr, frame_size=frame_size,\n",
    "                        NFFT=NFFT,\n",
    "                        fmin=fmin, n_bins=n_bins, \n",
    "                        bins_per_octave=bins_per_octave\n",
    "                    )\n",
    "                    \n",
    "                    features.append(combined_feature)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        \n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Lưu dữ liệu vào file .pkl nếu chưa tồn tại\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            joblib.dump(features, features_file)\n",
    "            joblib.dump(labels, labels_file)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Feature extraction time: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Features shape: {features.shape}, Labels shape: {labels.shape}\")\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Trích xuất đặc trưng kết hợp cho tập dữ liệu\n",
    "train_features, train_labels = load_features_from_directory(\n",
    "    train_path, sample_rate=22050, NFFT=512, fmin=None, n_bins=96, bins_per_octave=12,\n",
    "    output_dir=output_dir, dataset_type='train'\n",
    ")\n",
    "\n",
    "val_features, val_labels = load_features_from_directory(\n",
    "    val_path, sample_rate=22050, NFFT=512, fmin=None, n_bins=96, bins_per_octave=12,\n",
    "    output_dir=output_dir, dataset_type='val'\n",
    ")\n",
    "\n",
    "test_features, test_labels = load_features_from_directory(\n",
    "    test_path, sample_rate=22050, NFFT=512, fmin=None, n_bins=96, bins_per_octave=12,\n",
    "    output_dir=output_dir, dataset_type='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "val_features_scaled   = scaler.transform(val_features)\n",
    "test_features_scaled  = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.02 seconds\n",
      "KNN (FFT features) - Validation Accuracy: 96.97%\n",
      "KNN (FFT features) - Test Accuracy: 97.15%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_classifier.fit(train_features, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_knn = knn_classifier.predict(val_features)\n",
    "test_accuracy_knn = knn_classifier.predict(test_features)\n",
    "print(f\"KNN (FFT features) - Validation Accuracy: {accuracy_score(val_labels, val_pred_knn)*100:.2f}%\")\n",
    "print(f\"KNN (FFT features) - Test Accuracy: {accuracy_score(test_labels, test_accuracy_knn)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.01 seconds\n",
      "KNN (FFT features) - Validation Accuracy: 96.87%\n",
      "KNN (FFT features) - Test Accuracy: 97.05%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_classifier.fit(train_features_scaled, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_knn = knn_classifier.predict(val_features_scaled)\n",
    "test_accuracy_knn = knn_classifier.predict(test_features_scaled)\n",
    "print(f\"KNN (FFT features) - Validation Accuracy: {accuracy_score(val_labels, val_pred_knn)*100:.2f}%\")\n",
    "print(f\"KNN (FFT features) - Test Accuracy: {accuracy_score(test_labels, test_accuracy_knn)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 81.22 seconds\n",
      "Test Accuracy (SVM with RBF Kernel): 98.60%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "svm_rbf_classifier = SVC(C=100, kernel='rbf', degree=3, gamma=0.1, coef0=0.0, shrinking=True, \n",
    "                         probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                         max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=42)\n",
    "\n",
    "X_combined = np.vstack((train_features, val_features))\n",
    "y_combined = np.concatenate((train_labels, val_labels))\n",
    "\n",
    "# Huấn luyện mô hình SVM trên dữ liệu kết hợp\n",
    "svm_rbf_classifier.fit(X_combined, y_combined)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_svm_rbf = svm_rbf_classifier.predict(test_features)\n",
    "test_accuracy_svm = accuracy_score(test_labels, test_predictions_svm_rbf)\n",
    "print(f\"Test Accuracy (SVM with RBF Kernel): {test_accuracy_svm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 115.15 seconds\n",
      "Validation Accuracy (SVM with RBF Kernel): 97.46%\n",
      "Test Accuracy (SVM with RBF Kernel): 98.00%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "svm_rbf_classifier = SVC(C=100, kernel='rbf', degree=3, gamma=0.1, coef0=0.0, shrinking=True, \n",
    "                         probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                         max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=42)\n",
    "\n",
    "# Huấn luyện mô hình SVM\n",
    "svm_rbf_classifier.fit(train_features_scaled, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_svm_rbf = svm_rbf_classifier.predict(val_features_scaled)\n",
    "val_accuracy_svm_rbf = accuracy_score(val_labels, val_predictions_svm_rbf)\n",
    "print(f\"Validation Accuracy (SVM with RBF Kernel): {val_accuracy_svm_rbf * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_svm_rbf = svm_rbf_classifier.predict(test_features_scaled)\n",
    "scale_test_accuracy_svm = accuracy_score(test_labels, test_predictions_svm_rbf)\n",
    "print(f\"Test Accuracy (SVM with RBF Kernel): {scale_test_accuracy_svm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 9.04 seconds\n",
      "Logistic Regression (FFT features) - Validation Accuracy: 88.51%\n",
      "Logistic Regression (FFT features) - Test Accuracy: 88.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinhg\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "lr_classifier = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True, intercept_scaling=1, \n",
    "                                   class_weight=None, random_state=42, solver='lbfgs', max_iter=1500, multi_class='deprecated', \n",
    "                                   verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "lr_classifier.fit(train_features, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_lr = lr_classifier.predict(val_features)\n",
    "test_accuracy_lr = lr_classifier.predict(test_features)\n",
    "print(f\"Logistic Regression (FFT features) - Validation Accuracy: {accuracy_score(val_labels, val_pred_lr)*100:.2f}%\")\n",
    "print(f\"Logistic Regression (FFT features) - Test Accuracy: {accuracy_score(test_labels, test_accuracy_lr)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 85.66 seconds\n",
      "Logistic Regression (FFT features) - Validation Accuracy: 88.21%\n",
      "Logistic Regression (FFT features) - Test Accuracy: 88.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinhg\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "lr_classifier = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True, intercept_scaling=1, \n",
    "                                   class_weight=None, random_state=42, solver='saga', max_iter=1000, multi_class='deprecated', \n",
    "                                   verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "lr_classifier.fit(train_features_scaled, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_lr = lr_classifier.predict(val_features_scaled)\n",
    "scale_test_accuracy_lr = lr_classifier.predict(test_features_scaled)\n",
    "print(f\"Logistic Regression (FFT features) - Validation Accuracy: {accuracy_score(val_labels, val_pred_lr)*100:.2f}%\")\n",
    "print(f\"Logistic Regression (FFT features) - Test Accuracy: {accuracy_score(test_labels, scale_test_accuracy_lr)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 42.03 seconds\n",
      "Random Forest (FFT features) - Validation Accuracy: 94.63%\n",
      "Random Forest (FFT features) - Test Accuracy: 95.12%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                                       min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', \n",
    "                                       max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, \n",
    "                                       n_jobs=None, random_state=42, verbose=0, warm_start=False, class_weight=None, \n",
    "                                       ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "rf_classifier.fit(train_features, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_pred_rf = rf_classifier.predict(val_features)\n",
    "test_accuracy_rf = rf_classifier.predict(test_features)\n",
    "print(f\"Random Forest (FFT features) - Validation Accuracy: {accuracy_score(val_labels, val_pred_rf)*100:.2f}%\")\n",
    "print(f\"Random Forest (FFT features) - Test Accuracy: {accuracy_score(test_labels, test_accuracy_rf)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 254.77 seconds\n",
      "Validation Accuracy (Extra Trees): 95.92%\n",
      "Test Accuracy (Extra Trees): 96.55%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "et_classifier = ExtraTreesClassifier(n_estimators=200, criterion='gini', max_depth=30, min_samples_split=2, min_samples_leaf=1, \n",
    "                                     min_weight_fraction_leaf=0.0, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                     bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, warm_start=False, \n",
    "                                     class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "et_classifier.fit(train_features, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_et = et_classifier.predict(val_features)\n",
    "val_accuracy_et = accuracy_score(val_labels, val_predictions_et)\n",
    "print(f\"Validation Accuracy (Extra Trees): {val_accuracy_et * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_et = et_classifier.predict(test_features)\n",
    "test_accuracy_et = accuracy_score(test_labels, test_predictions_et)\n",
    "print(f\"Test Accuracy (Extra Trees): {test_accuracy_et * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 157.91 seconds\n",
      "Validation Accuracy (Extra Trees): 95.97%\n",
      "Test Accuracy (Extra Trees): 96.58%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "et_classifier = ExtraTreesClassifier(n_estimators=200, criterion='gini', max_depth=30, min_samples_split=2, min_samples_leaf=1, \n",
    "                                     min_weight_fraction_leaf=0.0, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                     bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, warm_start=False, \n",
    "                                     class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "# Huấn luyện mô hình Extra Trees với dữ liệu đã chuẩn hóa\n",
    "et_classifier.fit(train_features_scaled, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ validation\n",
    "val_predictions_et = et_classifier.predict(val_features_scaled)\n",
    "val_accuracy_et = accuracy_score(val_labels, val_predictions_et)\n",
    "print(f\"Validation Accuracy (Extra Trees): {val_accuracy_et * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá mô hình trên bộ testing\n",
    "test_predictions_et = et_classifier.predict(test_features_scaled)\n",
    "scale_test_accuracy_et = accuracy_score(test_labels, test_predictions_et)\n",
    "print(f\"Test Accuracy (Extra Trees): {scale_test_accuracy_et * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (Gradient Boosting): 14.44 seconds\n",
      "Validation Accuracy (Gradient Boosting): 90.00%\n",
      "Test Accuracy (Gradient Boosting): 91.55%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                            max_depth=3, min_samples_split=2,\n",
    "                            min_samples_leaf=1, subsample=1.0,\n",
    "                            max_features='sqrt', random_state=42)\n",
    "\n",
    "gb_classifier.fit(train_features_scaled, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time (Gradient Boosting): {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "val_predictions_gb = gb_classifier.predict(val_features_scaled)\n",
    "val_accuracy_gb = accuracy_score(val_labels, val_predictions_gb)\n",
    "print(f\"Validation Accuracy (Gradient Boosting): {val_accuracy_gb * 100:.2f}%\")\n",
    "\n",
    "test_predictions_gb = gb_classifier.predict(test_features_scaled)\n",
    "test_accuracy_gb = accuracy_score(test_labels, test_predictions_gb)\n",
    "print(f\"Test Accuracy (Gradient Boosting): {test_accuracy_gb * 100:.2f}%\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinhg\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\xgboost\\core.py:158: UserWarning: [22:00:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (XGBoost): 37.49 seconds\n",
      "Validation Accuracy (XGBoost): 96.92%\n",
      "Test Accuracy (XGBoost): 97.12%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Chuyển đổi các nhãn từ chuỗi sang số\n",
    "le = LabelEncoder()\n",
    "train_labels_enc = le.fit_transform(train_labels)\n",
    "val_labels_enc = le.transform(val_labels)\n",
    "test_labels_enc = le.transform(test_labels)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=30,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,    # Tắt cảnh báo về label encoder\n",
    "    eval_metric='logloss'       # Chỉ định hàm mất mát\n",
    ")\n",
    "xgb_classifier.fit(train_features, train_labels_enc)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time (XGBoost): {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá trên bộ validation\n",
    "val_predictions_xgb = xgb_classifier.predict(val_features)\n",
    "val_accuracy_xgb = accuracy_score(val_labels_enc, val_predictions_xgb)\n",
    "print(f\"Validation Accuracy (XGBoost): {val_accuracy_xgb * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá trên bộ testing\n",
    "test_predictions_xgb = xgb_classifier.predict(test_features)\n",
    "test_accuracy_xgb = accuracy_score(test_labels_enc, test_predictions_xgb)\n",
    "print(f\"Test Accuracy (XGBoost): {test_accuracy_xgb * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7000, number of negative: 7066\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90015\n",
      "[LightGBM] [Info] Number of data points in the train set: 14066, number of used features: 353\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497654 -> initscore=-0.009384\n",
      "[LightGBM] [Info] Start training from score -0.009384\n",
      "\n",
      "Training time (LightGBM): 2.54 seconds\n",
      "Validation Accuracy (LightGBM): 97.21%\n",
      "Test Accuracy (LightGBM): 97.10%\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lgbm_classifier = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=30,\n",
    "    random_state=42\n",
    ")\n",
    "lgbm_classifier.fit(train_features, train_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining time (LightGBM): {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Đánh giá trên bộ validation\n",
    "val_predictions_lgbm = lgbm_classifier.predict(val_features)\n",
    "val_accuracy_lgbm = accuracy_score(val_labels, val_predictions_lgbm)\n",
    "print(f\"Validation Accuracy (LightGBM): {val_accuracy_lgbm * 100:.2f}%\")\n",
    "\n",
    "# Đánh giá trên bộ testing\n",
    "test_predictions_lgbm = lgbm_classifier.predict(test_features)\n",
    "test_accuracy_lgbm = accuracy_score(test_labels, test_predictions_lgbm)\n",
    "print(f\"Test Accuracy (LightGBM): {test_accuracy_lgbm * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
